{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_Day3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pq9Jt4e6jjIP",
        "colab": {}
      },
      "source": [
        " import keras\n",
        "keras.__version__\n",
        "from keras.datasets import imdb\n",
        "import numpy as np\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from keras import metrics\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r7H_wgzEjjIe"
      },
      "source": [
        "# Homework Three\n",
        "\n",
        "This notebook is adapted from Chapter 3, Section 4 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff) by Francois Chollet. You will have seen Chapter 3, section 5 as a demonstration this morning.  You should read these sections as you work through the assignment.\n",
        "\n",
        "Your goal with this notebook is to carry out the execution of a machine learning program to classify text files.  Yesterday, you trained a machine to be able to identify certain images (e.g. as an apple, a banana, etc.).  Today the question is whether the computer can take a written reaction to a film and tell whether the reviewer overall had a positive or negative opinion of a movie.\n",
        "\n",
        "Along the way, we will emphasize the importance of shaping the data into a form that the computer can process.  Although your project will not likely use these exact steps, unless you choose a text-based project, it is highly unlikely that you won't have to reshape your data in some manner to get it into the program -- this worksheet will give you practice in one way of doing that.\n",
        "\n",
        "As usual, make sure your name is at the top of this.  You may work with a partner on this homework, if you wish.\n",
        "\n",
        "Make sure you run the first cell, above, to import all the necessary libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sUIEAcv0jjIg"
      },
      "source": [
        "## The IMDB dataset (This cell copied from Chollet)\n",
        "\n",
        "\n",
        "We'll be working with \"IMDB dataset\", a set of 50,000 highly-polarized reviews from the Internet Movie Database. They are split into 25,000 \n",
        "reviews for training and 25,000 reviews for testing, each set consisting in 50% negative and 50% positive reviews.\n",
        "\n",
        "Why do we have these two separate training and test sets? You should never test a machine learning model on the same data that you used to \n",
        "train it! Just because a model performs well on its training data doesn't mean that it will perform well on data it has never seen, and \n",
        "what you actually care about is your model's performance on new data (since you already know the labels of your training data -- obviously \n",
        "you don't need your model to predict those). For instance, it is possible that your model could end up merely _memorizing_ a mapping between \n",
        "your training samples and their targets -- which would be completely useless for the task of predicting targets for data never seen before. \n",
        "We will go over this point in much more detail in the next chapter.\n",
        "\n",
        "The IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) \n",
        "have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.\n",
        "\n",
        "The following code will load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q26i57TDjjIj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d23485dd-f436-4ae1-9acb-7252a60cbf32"
      },
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oz5N3ivIjjIp"
      },
      "source": [
        "From Chollet:\n",
        "\n",
        "The argument `num_words=10000` means that we will only keep the top 10,000 most frequently occurring words in the training data. Rare words \n",
        "will be discarded. This allows us to work with vector data of manageable size.\n",
        "\n",
        "The variables `train_data` and `test_data` are lists of reviews, each review being a list of word indices (encoding a sequence of words). \n",
        "`train_labels` and `test_labels` are lists of 0s and 1s, where 0 stands for \"negative\" and 1 stands for \"positive\":"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x-seZnQetlQp"
      },
      "source": [
        "Let's perform some operations to understand just what you've downloaded here.  According to Chollet's introduction, the total number of movie reviews has been split into two halves: one for training and one for testing.  If you print the length of the variables `train_data`, `train_labels`, `test_data`, and `test_labels`, how long should each of them be?  Carry out this check and see if the answers are what you expected:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8g2ou7mkjjIr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5aabe41e-ca3e-4ee4-c41e-ff1c92edaab0"
      },
      "source": [
        "len(train_data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L1XmEbPluHSL"
      },
      "source": [
        "<font color=red>Did you get what you expected?</font>\n",
        "\n",
        "Yes, since the dataset will be split into two there will be 25000 reviews to train with and  25000 reviews to test with. Since the command says len(train_data) it will display the amount of training data which is 25000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4KKWH3NzuSQ1"
      },
      "source": [
        "Now, let's look at what is in each of these variables:  try printing one of the training data examples.  Pick a number you like between 0 and 24,999."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HkaavCX_jjIx",
        "colab": {}
      },
      "source": [
        "mynumber = 24998"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6m3rCOpdupAB"
      },
      "source": [
        "Let's look at the training data and the label of your chosen example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N0dxjtnuu0sk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16721ff4-ce99-4ab7-b7d4-c7a784e99240"
      },
      "source": [
        "train_data[mynumber]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 1446,\n",
              " 7079,\n",
              " 69,\n",
              " 72,\n",
              " 3305,\n",
              " 13,\n",
              " 610,\n",
              " 930,\n",
              " 8,\n",
              " 12,\n",
              " 582,\n",
              " 23,\n",
              " 5,\n",
              " 16,\n",
              " 484,\n",
              " 685,\n",
              " 54,\n",
              " 349,\n",
              " 11,\n",
              " 4120,\n",
              " 2959,\n",
              " 45,\n",
              " 58,\n",
              " 1466,\n",
              " 13,\n",
              " 197,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 23,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 30,\n",
              " 145,\n",
              " 402,\n",
              " 11,\n",
              " 4131,\n",
              " 51,\n",
              " 575,\n",
              " 32,\n",
              " 61,\n",
              " 369,\n",
              " 71,\n",
              " 66,\n",
              " 770,\n",
              " 12,\n",
              " 1054,\n",
              " 75,\n",
              " 100,\n",
              " 2198,\n",
              " 8,\n",
              " 4,\n",
              " 105,\n",
              " 37,\n",
              " 69,\n",
              " 147,\n",
              " 712,\n",
              " 75,\n",
              " 3543,\n",
              " 44,\n",
              " 257,\n",
              " 390,\n",
              " 5,\n",
              " 69,\n",
              " 263,\n",
              " 514,\n",
              " 105,\n",
              " 50,\n",
              " 286,\n",
              " 1814,\n",
              " 23,\n",
              " 4,\n",
              " 123,\n",
              " 13,\n",
              " 161,\n",
              " 40,\n",
              " 5,\n",
              " 421,\n",
              " 4,\n",
              " 116,\n",
              " 16,\n",
              " 897,\n",
              " 13,\n",
              " 2,\n",
              " 40,\n",
              " 319,\n",
              " 5872,\n",
              " 112,\n",
              " 6700,\n",
              " 11,\n",
              " 4803,\n",
              " 121,\n",
              " 25,\n",
              " 70,\n",
              " 3468,\n",
              " 4,\n",
              " 719,\n",
              " 3798,\n",
              " 13,\n",
              " 18,\n",
              " 31,\n",
              " 62,\n",
              " 40,\n",
              " 8,\n",
              " 7200,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 14,\n",
              " 123,\n",
              " 5,\n",
              " 942,\n",
              " 25,\n",
              " 8,\n",
              " 721,\n",
              " 12,\n",
              " 145,\n",
              " 5,\n",
              " 202,\n",
              " 12,\n",
              " 160,\n",
              " 580,\n",
              " 202,\n",
              " 12,\n",
              " 6,\n",
              " 52,\n",
              " 58,\n",
              " 2,\n",
              " 92,\n",
              " 401,\n",
              " 728,\n",
              " 12,\n",
              " 39,\n",
              " 14,\n",
              " 251,\n",
              " 8,\n",
              " 15,\n",
              " 251,\n",
              " 5,\n",
              " 2,\n",
              " 12,\n",
              " 38,\n",
              " 84,\n",
              " 80,\n",
              " 124,\n",
              " 12,\n",
              " 9,\n",
              " 23]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k-WdUjuZvRfJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96c447d2-acfd-4dc9-8c77-754796d3a2f1"
      },
      "source": [
        "train_labels[mynumber]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2SEEXCe9vVFf"
      },
      "source": [
        "<font color=red>Describe your output and explain what it means</font>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This output means that the review selected was positive and the output displays 1 to indicate that the review is lablled as a positive review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iWM6nRjxjjI9"
      },
      "source": [
        "Now, let's convert your example review back to English so you can actually read it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aPfNGgYWjjI-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "95b33e19-d9c5-4d06-da5a-d4648df12306"
      },
      "source": [
        "# word_index is a dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index() # This variable links each word to a number\n",
        "# We reverse it, mapping integer indices to words\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "# We decode the review; note that our indices were offset by 3\n",
        "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[mynumber]])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5HKVjlayjjJC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "198d25ac-f5f0-479a-c0d8-9ac73a036d74"
      },
      "source": [
        "decoded_review\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"? six degrees had me hooked i looked forward to it coming on and was totally disappointed when men in trees replaced it's time spot i thought it was just on ? and would be back early in 2007 what happened all my friends were really surprised it ended we could relate to the characters who had real problems we talked about each episode and had our favorite characters there wasn't anybody on the show i didn't like and felt the acting was superb i ? like seeing programs being taped in cities where you can identify the local areas i for one would like to protest the ? of this show and ask you to bring it back and give it another chance give it a good time ? don't keep moving it from this day to that day and ? it so people will know it is on\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BZz3IJjBv0ZU"
      },
      "source": [
        "<font color=red>Would you say this is a positive or negative review?  Does that match the `training_label` value?</font>\n",
        "\n",
        "This review is postivie and it matches the training label since it labels it as 1 which means that it is positive.\n",
        "\n",
        "(Try running it for a few more numbers until you get both a positive and a negative review.  Can you find a movie you've seen?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T8HXkik6jjJG"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "The neural network software requires that the input be in a particular format.  The list of numbers you saw in `train_data` is not the correct format.  Instead of a list, we need a tensor.  For this problem, the way we are going to do that is make a tensor that has 25,000 rows and 10,000 columns (10,000 because we are using the 10,000 most common words) and is all filled with zeros.  Then, each column stands for a particular word.  Each row is a particular review.  If a particular word appears in the review, we turn the associated column to a one.\n",
        "\n",
        "Already you can see a semantic weakness in this approach -- we're training the network purely based on what words appear.  With this method, we lose all information about the order they appear as well as how many times they appear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sggPix1vjjJH",
        "colab": {}
      },
      "source": [
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
        "    return results\n",
        "\n",
        "# Our vectorized training data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# Our vectorized test data\n",
        "x_test = vectorize_sequences(test_data)\n",
        "# Our vectorized labels\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wrMGJEmKjjJM"
      },
      "source": [
        "See what your chosen training review looks like now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JAxV8dScjjJM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bf6c86e4-d4dd-44ff-d99b-9be16f0bb50a"
      },
      "source": [
        "print(np.shape(x_train))\n",
        "print(x_train[mynumber,:])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 10000)\n",
            "[0. 1. 1. ... 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "umGB3Mok3hcP"
      },
      "source": [
        "<font color=red>Explain in your own words what you have done here. </font>\n",
        "\n",
        "I have printed an array with 25000 rows and 10000 columns. The rows represent the number of reviews and the coolumns represent the 10,000 most common words. This array will turn a entire columns to 1s if a common word is present. \n",
        "\n",
        "<font color=red>Pick an example of a word in the review you have chosen and show step by step how that word ends up determining what part of `x_train`:</font> (in other words, something like \"the word `wonderful` was in my review.  It is the Xth word in the dictionary, so...\" (what happened with `train_data`, and then how did that affect `x_data`?)\n",
        "\n",
        "An example word can be \"hooked\" and if this word is present in the 10000 most common words then it will be the xth word in the dictionary and if this word is present the column associated with this word will become 1. I assume if positive word are found in a review the machine learning model will label it as positive. \n",
        "\n",
        "\n",
        "\n",
        "<font color=red>What does the variable `y_train` contain?</font>\n",
        "\n",
        "y train contains the train labels for the program."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wr7iRfIPjjJU"
      },
      "source": [
        "Now our data is ready to be fed into a neural network.\n",
        "\n",
        "First, we construct a very simple model of a three layer network.  For now, don't worry about the details of the layers, unless you want to dig into it yourself (see, e.g. the Chollet book, section 3.4, for more details)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GSPIRB-zjjJX",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0hGEmv0ejjJb"
      },
      "source": [
        "(This cell copied from Chollet)\n",
        "\n",
        "Lastly, we need to pick a loss function and an optimizer. Since we are facing a binary classification problem and the output of our network \n",
        "is a probability (we end our network with a single-unit layer with a sigmoid activation), is it best to use the `binary_crossentropy` loss. \n",
        "It isn't the only viable choice: you could use, for instance, `mean_squared_error`. But crossentropy is usually the best choice when you \n",
        "are dealing with models that output probabilities. Crossentropy is a quantity from the field of Information Theory, that measures the \"distance\" \n",
        "between probability distributions, or in our case, between the ground-truth distribution and our predictions.\n",
        "\n",
        "Here's the step where we configure our model with the `rmsprop` optimizer and the `binary_crossentropy` loss function. Note that we will \n",
        "also monitor accuracy during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "21CgonamjjJc",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lmkK-uxa4rE2"
      },
      "source": [
        "<font color=red>In your own words, what did the previous cell do?</font>\n",
        "\n",
        "The previous cell adds a loss function and optimizer to our model. The loss function or method is binary crossentropy which is best since we are dealing with probabilities and the optimizer is the rmsprop. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dwgCm8aCjjJm"
      },
      "source": [
        "# Validating our approach\n",
        "\n",
        "We need to have a way to assess whether the network is doing its job, while it is being built.  To achieve this goal, we set aside 10,000 examples out of the training set to be a ``validation set''.  The network will train on the 15,000 that are left, and then it will apply its model to the 10,000 in the validation set.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kl7jSft4jjJn",
        "colab": {}
      },
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mhbR6grg5spv"
      },
      "source": [
        "<font color=red>What is the difference between `x_val` and `partial_x_train`?</font>\n",
        "\n",
        "xval seems to to be the reviews in the validation set for the program and parial x train only trains a partial amount of the total set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "ddsjoqSXjjJq"
      },
      "source": [
        "# Training the Network\n",
        "\n",
        "Now, we train the network.  We can set the number of epochs (how many times to iterate).  We will start with 20.  We also need to have the computer pick out sub-samples from our training set to act on at a time.  It would take far too long to process 15,000 reviews all at once.  So we choose 512 as the batch size.  Finally, we pass the validation sample to the code for it to use as a test of how well the network performs.\n",
        "\n",
        "Remember, the loss function is essentially how far off your model is from ideal.  The accuracy function is what fraction of the model's prediction match what is in `y_train`.\n",
        "\n",
        "Let's try it!  Watch the four output numbers as it runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wcoMP3HGjjJq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "1bcc038d-e180-46e8-d807-33ce7751e661"
      },
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 0.4955 - accuracy: 0.7889 - val_loss: 0.3662 - val_accuracy: 0.8731\n",
            "Epoch 2/20\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.2965 - accuracy: 0.9050 - val_loss: 0.3115 - val_accuracy: 0.8798\n",
            "Epoch 3/20\n",
            "30/30 [==============================] - 1s 32ms/step - loss: 0.2183 - accuracy: 0.9279 - val_loss: 0.2992 - val_accuracy: 0.8779\n",
            "Epoch 4/20\n",
            "30/30 [==============================] - 1s 32ms/step - loss: 0.1736 - accuracy: 0.9439 - val_loss: 0.2735 - val_accuracy: 0.8899\n",
            "Epoch 5/20\n",
            "30/30 [==============================] - 1s 32ms/step - loss: 0.1384 - accuracy: 0.9569 - val_loss: 0.2812 - val_accuracy: 0.8869\n",
            "Epoch 6/20\n",
            "30/30 [==============================] - 1s 32ms/step - loss: 0.1164 - accuracy: 0.9650 - val_loss: 0.3171 - val_accuracy: 0.8779\n",
            "Epoch 7/20\n",
            "30/30 [==============================] - 1s 32ms/step - loss: 0.0953 - accuracy: 0.9702 - val_loss: 0.3095 - val_accuracy: 0.8825\n",
            "Epoch 8/20\n",
            "30/30 [==============================] - 1s 32ms/step - loss: 0.0810 - accuracy: 0.9755 - val_loss: 0.3291 - val_accuracy: 0.8839\n",
            "Epoch 9/20\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.0653 - accuracy: 0.9823 - val_loss: 0.3819 - val_accuracy: 0.8723\n",
            "Epoch 10/20\n",
            "30/30 [==============================] - 1s 32ms/step - loss: 0.0524 - accuracy: 0.9869 - val_loss: 0.4619 - val_accuracy: 0.8574\n",
            "Epoch 11/20\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.0459 - accuracy: 0.9892 - val_loss: 0.3994 - val_accuracy: 0.8783\n",
            "Epoch 12/20\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.0351 - accuracy: 0.9927 - val_loss: 0.4368 - val_accuracy: 0.8730\n",
            "Epoch 13/20\n",
            "30/30 [==============================] - 1s 32ms/step - loss: 0.0292 - accuracy: 0.9941 - val_loss: 0.4567 - val_accuracy: 0.8733\n",
            "Epoch 14/20\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.0237 - accuracy: 0.9953 - val_loss: 0.4906 - val_accuracy: 0.8697\n",
            "Epoch 15/20\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.0183 - accuracy: 0.9976 - val_loss: 0.5249 - val_accuracy: 0.8674\n",
            "Epoch 16/20\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.0161 - accuracy: 0.9969 - val_loss: 0.5502 - val_accuracy: 0.8682\n",
            "Epoch 17/20\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.0089 - accuracy: 0.9997 - val_loss: 0.5839 - val_accuracy: 0.8670\n",
            "Epoch 18/20\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.0093 - accuracy: 0.9991 - val_loss: 0.6291 - val_accuracy: 0.8639\n",
            "Epoch 19/20\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.0088 - accuracy: 0.9984 - val_loss: 0.6491 - val_accuracy: 0.8675\n",
            "Epoch 20/20\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.0042 - accuracy: 0.9999 - val_loss: 0.7300 - val_accuracy: 0.8630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FIq73Uop63Nu"
      },
      "source": [
        "<font color=red>Define, in your own words, what each of the four values (`loss`, `accuracy`, `val_loss`, `val_accuracy`) means, and what you observe the numbers doing over the course of the 20 epochs:</font>\n",
        "\n",
        "Loss is the number which repersents how poorly the model is performing or bad the model is at predicitng if a review is positive or negative. Accuracy displays number of correct predictions over total number of predictions. Accuracy displays the value of how good the model is a predicting if a review is good or bad. Val_loss and Val_accuracy are loss and acciracy numbers for purely the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FwYPr0hOjjJt"
      },
      "source": [
        "# Evaluating the results\n",
        "\n",
        "In the last cell, I asked you to describe what the four numbers did.  Now, let's get more quantitative about that.  We're going to graph them.  The model fit function returned a variable called `history`.  This is a record of how the model performed.  Let's pull the numbers out of it that we need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IlrVpzlqjjJy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "0c083a56-d659-4330-ccf0-25f86ae00120"
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'ro', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'bx-', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XHOUi_VJjjJ1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "6e69a57b-7002-4f08-bab1-3a5182878b48"
      },
      "source": [
        "plt.clf()   # clear figure\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'ro', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'bx-', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3s5clP71jjJ3"
      },
      "source": [
        "<font color=red>In this cell, I want you to describe and interpret what you see here.  There are four curves in two graphs.\n",
        "\n",
        "Both graphs have loss as y axis and the number of epochs as x axis. Additionally the initial graph only displays the training loss and validation loss. The training loss went down over the epochs but the validation loss increased with epochs. In the second graph the training accuracy curve starts near 0.8 and goes toward 1 indicating the curve for accuracy is going toward 100% accuracy. Howver the validation curve accuracy remianed overall stagnant.\n",
        "\n",
        "<P>First: what are the red dots telling you?  In other words, what do they represent, how do they change, and what do you infer from that change?\n",
        "\n",
        "The red dots in the initial graph represent loss number at a specific epoch and these go toward 0 indicating that the model is predicitng more reviews correctly compared to the start of the epochs. In the second graph the red dots represent the training accuracy. The training accuracy go towards 1.00 or 100 percent indicating that the model is predicing the data correctly.\n",
        "\n",
        "<P>Second: what does the blue line tell you?<P>Do these trends surprise you at all?</font>\n",
        "\n",
        "The blue lines are the accuracy and loss for the validation set. These lines were either stable such as the validation set one or had a downward curve like the initial one. These trends suprise me since I expected them to have similar pattern to regular loss and accuracy values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UR7_FyNm-Gco"
      },
      "source": [
        "The key point you should have recognized is that while the model gets better and better at matching the training data, that doesn't necessarily help it match the validation data.  The model is getting really good at identifying *these* movie reviews, but at a certain point, it can't do any better at classifying *other* movie reviews.\n",
        "\n",
        "<font color=red>At what epoch does this model perform best on the validation data?</font>\n",
        "\n",
        "At the forth epoch since the validation accuracy was the highest at that point 88% and the loss was at it's lowest compared to all other epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hroX0qaUjjJ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "aef0ef4b-f3a2-480a-e915-93d6c7479847"
      },
      "source": [
        "# Copy the code here that defines the three layers of the network:\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Copy the code here that compiles the loss and accuracy functions\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Copy the code here that trains the model, but instead of going all the way out to 20 epochs, stop at the epoch you think makes the most sense.\n",
        "best_epoch = 4 # Change this!!!\n",
        "model.fit(x_train, y_train, epochs=best_epoch, batch_size=512)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "49/49 [==============================] - 1s 26ms/step - loss: 0.4773 - accuracy: 0.8180\n",
            "Epoch 2/4\n",
            "49/49 [==============================] - 1s 26ms/step - loss: 0.2743 - accuracy: 0.9058\n",
            "Epoch 3/4\n",
            "49/49 [==============================] - 1s 26ms/step - loss: 0.2090 - accuracy: 0.9244\n",
            "Epoch 4/4\n",
            "49/49 [==============================] - 1s 26ms/step - loss: 0.1735 - accuracy: 0.9388\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2e053156d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ejeJNh84870N"
      },
      "source": [
        "Now you're going to test your model on the test data -- the 25,000 reviews we set aside at the beginning.  Your model hasn't see these data; it can't be biased by them in any way.  It will return the loss function and the accuracy of the model as it acts on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cSNQmjxDjjJ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7d3da479-d558-4961-b4f0-c295dfb9bc8e"
      },
      "source": [
        "results = model.evaluate(x_test, y_test)\n",
        "print(results)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 1s 2ms/step - loss: 0.2917 - accuracy: 0.8838\n",
            "[0.2916523516178131, 0.883840024471283]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xOac_beMjjJ9"
      },
      "source": [
        "<font color=red>How accurate was your model on the test data?</font>\n",
        "\n",
        "The model was relativly accurate since the accuracy number is high rangeing from 88 to 93 percent and the loss is low therefore this indicates the model is accuracte on the model test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D2gzpBg0jjKA"
      },
      "source": [
        "# Confidence of Prediction\n",
        "\n",
        "We have collapsed the prediction into a binary determination: was the review positive (1) or negative (0)?  But that's not actually what the model outputs.  It outputs a number between 0 and 1.  The closer its prediction is to either end, the more confident the model is in its classification.  Let's look at some of its predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "17PnZ7EAjjKA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "40ddb7bd-7cf9-48ea-9d28-8c4e9919324d"
      },
      "source": [
        "predictions = model.predict(x_test)\n",
        "print(np.shape(predictions))\n",
        "print('Review\\tLabel\\tPrediction')\n",
        "for i in np.arange(30):\n",
        "  print(i, '\\t',test_labels[i],'\\t',predictions[i]) \n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 1)\n",
            "Review\tLabel\tPrediction\n",
            "0 \t 0 \t [0.15234947]\n",
            "1 \t 1 \t [0.99953395]\n",
            "2 \t 1 \t [0.86021817]\n",
            "3 \t 0 \t [0.71160555]\n",
            "4 \t 1 \t [0.9390141]\n",
            "5 \t 1 \t [0.8281306]\n",
            "6 \t 1 \t [0.9981923]\n",
            "7 \t 0 \t [0.00341767]\n",
            "8 \t 0 \t [0.95245373]\n",
            "9 \t 1 \t [0.97894573]\n",
            "10 \t 1 \t [0.91903055]\n",
            "11 \t 0 \t [0.00439373]\n",
            "12 \t 0 \t [6.9863396e-05]\n",
            "13 \t 0 \t [0.02048954]\n",
            "14 \t 1 \t [0.99224985]\n",
            "15 \t 0 \t [1.1406256e-05]\n",
            "16 \t 1 \t [0.8871757]\n",
            "17 \t 0 \t [0.5667158]\n",
            "18 \t 0 \t [0.0023213]\n",
            "19 \t 0 \t [0.04066023]\n",
            "20 \t 1 \t [0.9947217]\n",
            "21 \t 1 \t [0.98982114]\n",
            "22 \t 1 \t [0.23108163]\n",
            "23 \t 1 \t [0.9146055]\n",
            "24 \t 1 \t [0.8210072]\n",
            "25 \t 1 \t [0.9641663]\n",
            "26 \t 0 \t [0.01848817]\n",
            "27 \t 1 \t [0.93577397]\n",
            "28 \t 1 \t [0.95173085]\n",
            "29 \t 0 \t [1.51455315e-05]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ruwu2bITjjKC"
      },
      "source": [
        "As you can see, the network is very confident for some samples (0.99 or more, or 0.01 or less) but less confident for others (0.6, 0.4). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Iy-yr1im_9nS"
      },
      "source": [
        "<font color=red>Your last task: pick at least four examples above.  One should have a very high predictive number, one should have a very low predictive number, and two should be as close to 0.5 as you can find.  Use the code above (copy it into a code cell below -- don't change what you have up there) to convert those reviews back into text and see if you agree with your neural network.  Find a case where the neural network was wrong, and convert that review back to text.  How would *you* label that review?</font>\n",
        "\n",
        "I could not find an example of the neural network in the 4 selected examples. However, i am not sure about the fisrt one the review could be negative looking at the words however, I cannot tell since the text is not in order. Therefore the first one could have been lablled incorrectly and it is actually negative or 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2IicxvMbVNX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "73316077-18c2-4502-fdd5-12f8a2b74bfc"
      },
      "source": [
        "#High prediciton number\n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 4, '?') for i in test_data[4]])\n",
        "print(decoded_review)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "? from if people into copy back and york fun stinks father the it called i side is is i side kill created classic movie cgi were about three in was favorite was not stupid better widmark ? hill you sure the ? an get justify not ? better less days of ? back on mountains i side to do ? the to loud family is is of often i cinematic difficult it his of often even dirty east just government movie s budget i side shine he three in is is only stay stay own work plays about this what ? work basically ? work deal work\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzOcug-rczt9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5b3dca3c-9f2a-4ecc-826d-c642d16093f3"
      },
      "source": [
        "#Low prediciton number \n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 15, '?') for i in test_data[15]])\n",
        "print(decoded_review)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "? next even than full by film body dvd by ? i'm ? great like a ? to at depicted ? needed go was ? ? ? audience ? actor ? rowlands ? ? to ? ? somewhat ? ? effects talent ? fools ? ? the delivers likes ? movies the soon ? think the soon ? to the grade either film's have ? quest of drake kumar ? quest ? whimsical acting would ? and my ? make triumph the are bolivia ? script remembered has watching if offering some strange ? watching if being he understand the triumph flying the without or as soundtrack that states caring ? fighting they i time power ? ? the mood ? tony or ? awful ? to no comedy your ? cried once ? critics doubt be ? ? how make ? ? seeing young you school ? obvious ? good through ? do even if carla on ? all ? to the ? down ? ? danny the ? ? depict script can ? to ? the scary such drug ? ? shoot head civil rather you the took ? ? to man for madness ? run a alike al least ? stan notorious out at it's coming it's looks his ? to plot no ? ? gore ? campy ? to ? are really ? family teen ? line ? stupidity since to ? under ? the acting affected effects ? ben the acting ? incoherent ? show ? i've bitch the acting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4T0ilWKdRtu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "aad5b71e-827a-4707-8d5d-44bf7ac0d67d"
      },
      "source": [
        "# Near 0.5\n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 17, '?') for i in test_data[17]])\n",
        "print(decoded_review)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "? deliver ? astaire ? ? patrick on she's the i money with so ? ? couple of featured virgin ? richard show i cameo ? wig their sound they of hit ? episodes arrives considers ? i money whole they than that lesbian spite ? turns ? i religious against ? ? ? at ? clips believe ? powerful ? ? like ? ? ? ? due satire consequences continues yourself ? nature ? found are so ? ? dark budget deliver ? ? break ? ? mess racism ? those govinda ? when naked lot ? ? mob ? ? ? performers excruciatingly ? bat ? story i development is ? ? ? especially so ? all ? setting references deliver ? ? series chosen ? ? ? ? chances understands due states deed you ? junior proof pitiful ? deliver done he thing characters and ? adam by that first quite i everyone many ? ? world want ? powerful major ? his so ? ? ? that's genre the given won't what movies ? boy ? going bent given a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZGchf43ddUP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c9f6256a-2bb2-4975-d7b1-b5ec5fcf476b"
      },
      "source": [
        "#Near 0.5\n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in test_data[3]])\n",
        "print(decoded_review)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "? i generally love this type of movie however this time i found myself wanting to kick the screen since i can't do that i will just complain about it this was absolutely idiotic the things that happen with the dead kids are very cool but the alive people are absolute idiots i am a grown man pretty big and i can defend myself well however i would not do half the stuff the little girl does in this movie also the mother in this movie is reckless with her children to the point of neglect i wish i wasn't so angry about her and her actions because i would have otherwise enjoyed the flick what a number she was take my advise and fast forward through everything you see her do until the end also is anyone else getting sick of watching movies that are filmed so dark anymore one can hardly see what is being filmed as an audience we are ? involved with the actions on the screen so then why the hell can't we have night vision\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YLp85X2CCAnF",
        "colab": {}
      },
      "source": [
        "# For example, when I ran this, review #3 was labeled 0, but the computer calculated 0.918!\n",
        "# Let's see what that looks like.\n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in test_data[3]])\n",
        "print(decoded_review)\n",
        "# For your choices, change the 3 into some other number."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cMcBY1zKjjKD"
      },
      "source": [
        "# Further experiments, if you want to dig deeper.\n",
        "\n",
        "(Copied from Chollet)\n",
        "\n",
        "* We were using 2 hidden layers. Try to use 1 or 3 hidden layers and see how it affects validation and test accuracy.\n",
        "* Try to use layers with more hidden units or less hidden units: 32 units, 64 units...\n",
        "* Try to use the `mse` loss function instead of `binary_crossentropy`.\n",
        "* Try to use the `tanh` activation (an activation that was popular in the early days of neural networks) instead of `relu`.\n",
        "\n",
        "These experiments will help convince you that the architecture choices we have made are all fairly reasonable, although they can still be \n",
        "improved!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I4OsGHHxjjKD"
      },
      "source": [
        "# Conclusions\n",
        "\n",
        "(Copied from Chollet)\n",
        "\n",
        "Here's what you should take away from this example:\n",
        "\n",
        "* There's usually quite a bit of preprocessing you need to do on your raw data in order to be able to feed it -- as tensors -- into a neural \n",
        "network. In the case of sequences of words, they can be encoded as binary vectors -- but there are other encoding options too.\n",
        "* Stacks of `Dense` layers with `relu` activations can solve a wide range of problems (including sentiment classification), and you will \n",
        "likely use them frequently.\n",
        "* In a binary classification problem (two output classes), your network should end with a `Dense` layer with 1 unit and a `sigmoid` activation, \n",
        "i.e. the output of your network should be a scalar between 0 and 1, encoding a probability.\n",
        "* With such a scalar sigmoid output, on a binary classification problem, the loss function you should use is `binary_crossentropy`.\n",
        "* The `rmsprop` optimizer is generally a good enough choice of optimizer, whatever your problem. That's one less thing for you to worry \n",
        "about.\n",
        "* As they get better on their training data, neural networks eventually start _overfitting_ and end up obtaining increasingly worse results on data \n",
        "never-seen-before. Make sure to always monitor performance on data that is outside of the training set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3r8PldHZ84Ut"
      },
      "source": [
        "<font color=red>As always, summarize in your own words what you learned from this exercise.</font>\n",
        "\n",
        "I learned that it is best to split your dataset to have both training and testing datasets. Additionally, I learned the importance of the validation set and learned how to create graphs to visualize this data. Also, I learned what binary crossentopy is and how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lyhzh4zI897j",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}